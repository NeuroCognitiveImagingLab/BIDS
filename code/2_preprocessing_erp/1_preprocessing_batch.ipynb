{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c88bcae9-2be4-42d0-965f-c37d3a0f1523",
   "metadata": {},
   "source": [
    "# EEG-ERP Preprocessing \n",
    "## Batch script\n",
    "\n",
    "This script will run on all subjects in `rawdata`. Steps performed include:\n",
    "- filtering\n",
    "- epoching\n",
    "- mark (but don't correct) bad segments of data (trials/channels) using AutoReject\n",
    "- pass the marked data to ICA\n",
    "- auto-idenitfy and remove ocular indepdent components \n",
    "- apply ICA to epoched data\n",
    "- apply AutoReject to ICA-cleaned data\n",
    "- rereference\n",
    "- export clean epochs to `.fif` file in `derivatives/erp_preprocessing`\n",
    "- save an MNE report with details/vizualizations of the above steps in `derivatives/erp_preprocessing/logs`\n",
    "\n",
    "Most parameters that you would want to change are read from `config.yml`. \n",
    "\n",
    "It is recommended that you not apply any baseline correction at this stage (and the defult config.yml file reflects this). Baseline correction can be applied in subsequent scripts, but doing baseline correction here precludes later using baseline regression.\n",
    "\n",
    "Bad channels and non-ocular independent components can be manually identified after this script is run the first time. These can be stored in additional config files saved in `rawdata`, and then this script can be re-run to have those excluded. In an ideal world with reasnably clean data this should not be necessary, but e.g., if there are broken electrodes, EGI data, data from children, etc. it may be necessary.\n",
    "\n",
    "---\n",
    "Copyright 2023 [Aaron J Newman](https://github.com/aaronjnewman), [NeuroCognitive Imaging Lab](http://ncil.science), [Dalhousie University](https://dal.ca)\n",
    "\n",
    "Released under the [The 3-Clause BSD License](https://opensource.org/licenses/BSD-3-Clause)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39db9a01-dd5f-4cc7-8607-2fc0be02abb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path as op\n",
    "from os import remove\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from yaml import CLoader as Loader\n",
    "import numpy as np\n",
    "import mne\n",
    "from mne.preprocessing import annotate_amplitude\n",
    "mne.set_log_level('error')\n",
    "from mne_bids import BIDSPath, read_raw_bids\n",
    "from scipy.stats import zscore\n",
    "from autoreject import Ransac, get_rejection_threshold, AutoReject\n",
    "from time import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "056b8946-693c-4056-97e0-51a3e4cf37ce",
   "metadata": {},
   "source": [
    "## Read Parameters from config.yml\n",
    "\n",
    "Will import study-level parameters from `config.yml` in `bids_root`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed2575b-4e47-4225-b250-dea80acf202e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this shouldn't change if you run this script from its default location in code/import\n",
    "bids_root = '../..'\n",
    "\n",
    "cfg_file = op.join(bids_root, 'config.yml')\n",
    "with open(cfg_file, 'r') as f:\n",
    "    config = yaml.load(f, Loader=Loader)\n",
    "\n",
    "study_name = config['study_name']\n",
    "task = config['task']\n",
    "data_type = config['data_type']\n",
    "eog = config['eog']\n",
    "\n",
    "### ADD IF STATEMENT IN CASE THIS NO WORK\n",
    "if config['drop_ch'] != None:\n",
    "    drop_ch = [s.strip() for s in config['drop_ch'].split(\",\")]  \n",
    "\n",
    "\n",
    "montage_fname = config['montage_fname']\n",
    "\n",
    "# fix per changes to config\n",
    "n_jobs = config['preprocessing_settings']['n_jobs']\n",
    "filt_p = {k: v for d in config['preprocessing_settings']['filter'] for k, v in d.items()}\n",
    "ica_p = {k: v for d in config['preprocessing_settings']['ica'] for k, v in d.items()}\n",
    "epoch_p = {k: v for d in config['preprocessing_settings']['epoch'] for k, v in d.items()}\n",
    "reject = epoch_p['reject']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1336b79f",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d2b4fc-7c40-4a7d-b529-717355fd03e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_path = op.join(bids_root, 'rawdata')\n",
    "\n",
    "derivatives_path = op.join(bids_root, 'derivatives', 'erp_preprocessing')\n",
    "if Path(derivatives_path).exists() == False:\n",
    "    Path(derivatives_path).mkdir(parents=True)\n",
    "\n",
    "report_path = op.join(derivatives_path, 'logs')\n",
    "if Path(report_path).exists() == False:\n",
    "    Path(report_path).mkdir(parents=True)\n",
    "\n",
    "\n",
    "epochs_suffix = '-epo.fif'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "12f6fb7e",
   "metadata": {},
   "source": [
    "### Event codes - mappings between values and labels\n",
    "\n",
    "Read `rawdata/event_code_mappings.yml` to get the labels for event codes for each experiment. This file is used to create the `event_id_map` dictionary.\n",
    "\n",
    "The `extra_mappings` defined in the cell below provides additional code mappings codes within this script. This is intended to be used to define event types that might be created in this script. For example, if the EEG file contains codes for stimulus types, and codes on each trial indicating whether a response was correct or not, then in this script we might want to define aadditional codes so that we can create event types for correct and incorrect trials of each condition seprately.\n",
    "\n",
    "These two dictionaries are combined to create the `event_id_map` dictionary, which is used to create the `event_id` column in the `events` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1381f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open( op.join(raw_path, 'event_code_mappings.yml'), 'r') as f:\n",
    "    code_map = yaml.load(f, Loader=Loader)\n",
    "    \n",
    "# add an entry for 0, because it can appear in OpenVibe data\n",
    "extra_mappings = {'NULL':0, \n",
    "                  'match/correct':2011, 'match/incorrect':2010, 'match/noresp':2012,\n",
    "                  'mismatch/correct':2001, 'mismatch/incorrect':2000, 'mismatch/noresp':2002,\n",
    "                  }\n",
    "\n",
    "# The line below combines the codes read from the file, and codes defined here\n",
    "# comment it out if you are only using one or the other\n",
    "event_id_map = {**code_map, **extra_mappings}\n",
    "\n",
    "# contingent_events could recode stimulus types ('targets') based on whether they were \n",
    "#  followed by a correct response (contingencies)\n",
    "proc_contingent_events = False # change to True to use this feature\n",
    "contingent_events = {'match/correct':{'target_code':event_id_map['match/noresp'], \n",
    "                                     'contingency_code':event_id_map['_corr'], \n",
    "                                     'contingency_lag':1\n",
    "                                     },\n",
    "                     }\n",
    "\n",
    "def proc_contingent_events():\n",
    "     for c, vals in contingent_events.items():\n",
    "        events_a = np.where(events[:, 2] == vals['target_code'])[0]\n",
    "        events_b_idx = events_a + vals['contingency_lag']\n",
    "        events_b = events[events_b_idx, 2] == vals['contingency_code']\n",
    "        events[events_a[events_b], 2] = event_id_map[c]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b134bcd2-1736-40bf-9514-7baa20e945c2",
   "metadata": {},
   "source": [
    "### Subject list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6d4d45-eebc-4f62-9902-cd5dc6a47dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'sub-'\n",
    "subjects = sorted([s[-7:] for s in glob(raw_path + '/' + prefix + '*')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92afb297",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dfafb8ae-e633-412e-824a-d1b7823be04e",
   "metadata": {},
   "source": [
    "## Read manually-marked independent components\n",
    "Run this script once, then inspect ICs (in the `sub-xxx.html` file that is stored in `derivatives/erp_preprocessing/reports` folder). Based on this, make decisions about whether any additional ICs should be added, or any automatically-removed ICs should be included. Add these to the `participants_manual_ic.yml` file located in the present folder. Then, run this script again to apply the changes.\n",
    "\n",
    "Additional ICs to remove were selected based on:\n",
    "- participants for whom more than 15% of trials were removred by AutoReject after ICA correct\n",
    "- participants for whom the average across all trials and all electrodes did not show a clear pattern of P1-N1-P2 components\n",
    "- for such participants, the scalp map and details of each IC were visuall inspected. Components were removed if they were\n",
    "    - focal at a single electrode, or a very low number of electrodes\n",
    "    - focal at the edges of the electrode montage\n",
    "    - present on a low number of trials\n",
    "    - showed no systematic pattern across trials that was time-locked to stimulus onset\n",
    "    \n",
    "ICs to add (un-remove) were selected based on:\n",
    "- IC shows clear and consistent temporal dependency on stimulus onset\n",
    "- IC appears to contain P1-N1-P2 complex, or part thereof\n",
    "- IC has broad scalp distribution across many electrodes, characteristic of ERP component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2a3942-be6b-408e-bb2c-9d881ebe15e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_file = './participants_manual_ic.yml'\n",
    "with open(cfg_file, 'r') as f:\n",
    "    ica_manual = yaml.load(f, Loader=Loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c16c2c",
   "metadata": {},
   "source": [
    "## Handle study Metadata\n",
    "\n",
    "Read behavioural log file to obtain trial metadata. This will need editing (or simply ignore) for every study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023fb2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_metadata = False # change to True if you have behavioural data you want to add to the EEG data output by this script\n",
    "\n",
    "# list columns in log file that we do not want to include in the metadata\n",
    "# add to this according to your study; typically there are numerous columns that are not relevant to the EEG data\n",
    "beh_drop_cols = ['frameRate', 'expName', \n",
    "                        'session', 'participant'\n",
    "                        ]\n",
    "\n",
    "# read behavioural log file to obtain trial metadata\n",
    "#this will need editing (or simply IGNORE) for every study\n",
    "def get_metadata():\n",
    "    log_path = op.join(raw_path, subject, 'ses-' + ses, 'beh')\n",
    "    log_file = log_path + '/' + subject + '_ses-' + ses + '_task-' + task + '_beh.tsv'\n",
    "    metadata = pd.read_csv(log_file)\n",
    "    # drop practice and other non-trials\n",
    "    metadata = metadata[~metadata['blockSort'].isna()]\n",
    "    # combine trial counter and index columns from the two blocks\n",
    "    # metadata['trial_num'] = metadata['trials.thisTrialN'].combine_first(metadata['trials_2.thisTrialN'])\n",
    "    # metadata['trial_index'] = metadata['trials.thisIndex'].combine_first(metadata['trials_2.thisIndex'])\n",
    " \n",
    "    # drop unneeded columns\n",
    "    metadata.drop(columns=beh_drop_cols, inplace=True)\n",
    "\n",
    "    # check if there are more events than metadata rows\n",
    "    # (in which case EEG data was saved for practice trials, which we want to drop)\n",
    "    if events.shape[0] > metadata.shape[0]:\n",
    "        n_practice_events = events.shape[0] - metadata.shape[0]\n",
    "        events = events[n_practice_events:]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "696f9344-3bcc-49eb-b322-c22bdecb0fce",
   "metadata": {},
   "source": [
    "## Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5532f7d5-4e01-4216-9135-ce2bedec277c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "rej_log_list = []\n",
    "\n",
    "for subject in subjects:\n",
    "    for ses in sessions:\n",
    "\n",
    "        start_time = time()\n",
    "        print('\\n-------------------------')\n",
    "        print('-------- ' + subject + ' --------')\n",
    "        print('-------------------------')\n",
    "\n",
    "        report = mne.Report(subject=subject, \n",
    "                            title=study_name + ' preprocessing: ' + subject + ' ' + ses,\n",
    "                            verbose='WARNING')\n",
    "\n",
    "        ### subject-specific paths\n",
    "        in_path = BIDSPath(root=raw_path, \n",
    "                        subject=subject[-3:],\n",
    "                        session=ses,\n",
    "                        datatype=data_type,\n",
    "                        task=task\n",
    "                        )\n",
    "\n",
    "        ### Import data\n",
    "        raw = read_raw_bids(in_path)\n",
    "        if 'drop_ch' in locals():\n",
    "            raw = raw.drop_channels(drop_ch)\n",
    "        else:\n",
    "            # foolproofing\n",
    "            n_chan = len(raw.info['chs'])\n",
    "            if n_chan > 40:\n",
    "                print('WARNING: ' + subject + ' has ' + n_chan + ' channels, but drop_ch is not defined in config.yml')\n",
    "                print('If you used 32 channels in the booth room you need to define drop_ch.')\n",
    "\n",
    "        ### Read events\n",
    "        ### THIS PROBABLY WON'T WORK OUT OF THE BOX\n",
    "        ### PROVED HARD TO DEVELOP ROBUST CODE FROM KLUGEY WAYS PREVIOUS STUDIES HANDLED\n",
    "        ### LEFT AS A PROBLEM TO SOLVE FOR FUTURE AARON (OR SOME OTHER BRAVE SOUL)\n",
    "        events, event_dict = mne.events_from_annotations(raw)\n",
    "\n",
    "        # remove event code(s) assocaited with '__', which marks start of recording but otherwise useless\n",
    "        if '__' in event_dict.keys():\n",
    "            events = np.delete(events, list(np.where(events[:, 2] == event_dict['__'])[0]), axis=0)\n",
    "        \n",
    "        # handle event codes from openVibe vs. other recording software\n",
    "        if 'OVTK' in list(event_dict.keys())[0]:\n",
    "            if list(event_dict.keys())[0].split('/')[1].split('_')[0] == 'OVTK':\n",
    "                # OpenVibe exports codes in hex format with a bunch of leading text, and then MNE/BIDS \n",
    "                # maps these to a sequence of numbers starting at 10001. \n",
    "                # This will convert the events array to the original codes sent by the stimulus program.\n",
    "                hex_dict = {int(str(k).split('_')[-1], 16):v for k, v in event_dict.items()}\n",
    "                codes_conv = np.copy(events[:, 2])\n",
    "                for k, v in hex_dict.items():\n",
    "                    codes_conv[codes_conv==v] = k\n",
    "                events[:, 2] = codes_conv \n",
    "\n",
    "        # Map labels to condition codes\n",
    "        event_id = {}\n",
    "        try:\n",
    "            for label, code in event_id_map.items():\n",
    "                event_id[label] = event_dict[label]\n",
    "        except:\n",
    "            event_id = event_id_map       \n",
    "        \n",
    "        # remove event code == 0\n",
    "        events = events[np.nonzero(events[:, 2])]\n",
    "\n",
    "        # remove duplicate events\n",
    "        events = np.unique(events, axis=0)\n",
    "\n",
    "        ## Here you could add study-specific event code processing, e.g.: recode events contingent on other events \n",
    "        if proc_contingent_events == True:\n",
    "            contingent_events()\n",
    "            \n",
    "        ## Add events to report\n",
    "        report.add_events(events, event_id=event_id, \n",
    "                            sfreq=raw.info['sfreq'],\n",
    "                            title='Events'\n",
    "                            )\n",
    "        plt.close()\n",
    "\n",
    "        # Mark any flat chanels/segments\n",
    "        # Note - seems to replace annotations, so run after event code processing \n",
    "        # (but before filtering or segmentation)\n",
    "        annotations, bads = annotate_amplitude(raw, \n",
    "                                            flat=0., \n",
    "                                            bad_percent=25,\n",
    "                                            )\n",
    "        raw.set_annotations(annotations)\n",
    "    \n",
    "        ### Filtering\n",
    "        # channel selection\n",
    "        picks = mne.pick_types(raw.info, \n",
    "                            eeg=True,\n",
    "                            eog=True\n",
    "                            )\n",
    "        \n",
    "        print('Filtering...')\n",
    "        job_start = time()\n",
    "        \n",
    "        # Filter for ICA  \n",
    "        raw_ica = raw.load_data().copy().filter(filt_p['l_freq_ica'], filt_p['h_freq'],\n",
    "                                    picks=picks,\n",
    "                                    n_jobs=n_jobs\n",
    "                                )\n",
    "\n",
    "        ## Filter for final\n",
    "        raw.filter(filt_p['l_freq'], filt_p['h_freq'],\n",
    "                picks=picks,\n",
    "                n_jobs=n_jobs\n",
    "                )\n",
    "        \n",
    "        print('Filtering took ' + str (time() - job_start) + ' s')\n",
    "        ## Add raw to report\n",
    "        report.add_raw(raw=raw, \n",
    "                psd=True, butterfly=True, \n",
    "                title='Raw data, bandpass filtered ' + str(filt_p['l_freq']) + '–' + str(filt_p['h_freq'])\n",
    "                )\n",
    "        # plots are generated that accumulate and hog resources. Close them to minimize this\n",
    "        plt.close()\n",
    "        \n",
    "        ### Epoch data filtered for ICA\n",
    "        epochs_ica = mne.Epochs(raw_ica,\n",
    "                                events, event_id,\n",
    "                                epoch_p['tmin'], epoch_p['tmax'],\n",
    "                                baseline=epoch_p['baseline'], detrend=epoch_p['detrend'],\n",
    "                                reject=None, \n",
    "                                flat=epoch_p['flat'],\n",
    "                                preload=True\n",
    "                            )\n",
    "\n",
    "        \n",
    "        # use AutoReject to remove bad epochs, repair sensors and return clean epochs.\n",
    "        print('AutoReject pre-ICA...')\n",
    "        job_start = time()\n",
    "        ar = AutoReject(n_interpolate=[1, 2, 4, 8, 16],\n",
    "                        random_state=ica_p['ica_random_state'],\n",
    "                        picks=mne.pick_types(epochs_ica.info, eeg=True, eog=False),\n",
    "                        n_jobs=n_jobs, \n",
    "                        verbose=False\n",
    "                    )\n",
    "        ar.fit(epochs_ica)\n",
    "        plt.close()\n",
    "\n",
    "        print('n_interpolate = ' +  str(ar.n_interpolate_['eeg']))\n",
    "        print('AutoReject took ' + str (time() - job_start) + ' s')\n",
    "        \n",
    "        reject_log = ar.get_reject_log(epochs_ica)\n",
    "        fig = reject_log.plot('horizontal', show=False);\n",
    "        report.add_figure(fig=fig, title='AutoReject log')\n",
    "\n",
    "\n",
    "        ### Fit ICA\n",
    "        print('ICA...')\n",
    "        job_start = time()\n",
    "        ica = mne.preprocessing.ICA(method=ica_p['ica_method'],\n",
    "                                    n_components=ica_p['n_components'],\n",
    "                                    random_state=ica_p['ica_random_state'],\n",
    "                                    max_iter='auto')\n",
    "        \n",
    "        ica.fit(epochs_ica[~reject_log.bad_epochs],  # added [~reject_log.bad_epochs] for AutoReject\n",
    "                decim=3, \n",
    "                picks=['eeg']\n",
    "                );\n",
    "\n",
    "        # Identify ocular ICs\n",
    "        # The default *z* threshold doesn't work for\n",
    "        # all subjects. This routine starts with the default z (from config) and steps down\n",
    "        # until at least n_max_eog EOG components are identified.\n",
    "        # The limitations of this are that it assumes there will always be at least n_max_eog EOG\n",
    "        # components (blinks are always present, but horizontal movements are not\n",
    "        # always present), and may not work if there are > 3 components, if the\n",
    "        # score of the third is > `z_step` less than the score of the second.\n",
    "        # In practice, many of these components (with EGI data) may not be ocular, but are (hopefully) not EEG.\n",
    "        # Be sure to check the reports and confirm no ERP components are rejected!\n",
    "\n",
    "        ica.exclude = []\n",
    "        num_excl = 0\n",
    "        z_thresh = ica_p['ica_zthresh'] \n",
    "        z_step = ica_p['ica_zstep']\n",
    "\n",
    "        while num_excl < ica_p['n_max_eog'] and z_thresh > 0:\n",
    "            eog_indices, eog_scores = ica.find_bads_eog(epochs_ica, threshold=z_thresh)\n",
    "            num_excl = len(eog_indices)\n",
    "            z_thresh -= z_step # won't impact things if num_excl is ≥ n_max_eog \n",
    "\n",
    "        ica.exclude = eog_indices\n",
    "        z_thresh_final = round(z_thresh + z_step, 2)\n",
    "\n",
    "        # Manual removal/re-addition of ICs based on visual inspection\n",
    "        if subject in ica_manual:\n",
    "            if 'manually_excluded_ic' in ica_manual[subject][ses]:\n",
    "                for ic in ica_manual[subject][ses]['manually_excluded_ic']:\n",
    "                    ica.exclude.append(ic)\n",
    "            if 'manually_included_ic' in ica_manual[subject][ses]:\n",
    "                for ic in ica_manual[subject][ses]['manually_included_ic']:\n",
    "                    ica.exclude.remove(ic)          \n",
    "\n",
    "        # Create average of EOG events\n",
    "        eog_evoked = mne.preprocessing.create_eog_epochs(raw_ica).average().apply_baseline(baseline=(None, epoch_p['tmin']))\n",
    "\n",
    "        ## Add ICA to report\n",
    "        report.add_ica(ica=ica, title='ICA', inst=epochs_ica,\n",
    "                    eog_evoked=eog_evoked, \n",
    "                    eog_scores=eog_scores,\n",
    "                    n_jobs=n_jobs\n",
    "                    )\n",
    "        print('ICA took ' + str (time() - job_start) + ' s')\n",
    "        plt.close()   \n",
    "            \n",
    "        ### Segment filtered raw data into epochs for final analysis\n",
    "        epochs = mne.Epochs(raw,\n",
    "                            events, event_id,\n",
    "                            epoch_p['tmin'], epoch_p['tmax'],\n",
    "                            baseline=epoch_p['baseline'], detrend=epoch_p['detrend'],\n",
    "                            reject=reject, \n",
    "                            flat=epoch_p['flat'],\n",
    "                            preload=True\n",
    "                        )\n",
    "\n",
    "        ### ----- Combine metadata with epochs\n",
    "        # Note that this uses the selection property of the epochs, which keeps track of the \n",
    "        #  original event indexes, even after some trials/epochs are dropped (e.g., if there were bad/flat channels)\n",
    "        if use_metadata == True:\n",
    "            epochs.metadata = metadata.iloc[epochs.selection]        \n",
    "        \n",
    "        ### Apply ICA correction to epochs\n",
    "        ica.apply(epochs)\n",
    "        \n",
    "        ### Apply AutoReject to further clean epochs\n",
    "        print('AutoReject post-ICA...')\n",
    "        job_start = time()    \n",
    "        epochs_clean, reject_log = ar.fit_transform(epochs, return_log=True)\n",
    "        print('AutoReject took ' + str (time() - job_start) + ' s')\n",
    "        plt.close()\n",
    "\n",
    "        fig = reject_log.plot('horizontal', show=False)\n",
    "        report.add_figure(fig=fig, title='AutoReject log')\n",
    "        plt.close()\n",
    "\n",
    "        ### Re-reference, now that channels are cleaned\n",
    "        epochs_clean.set_eeg_reference(ref_channels=epoch_p['rereference']);\n",
    "        \n",
    "        ### Save cleaned epochs      \n",
    "        out_path = BIDSPath(root=derivatives_path, \n",
    "                           subject=subject[-3:], \n",
    "                           datatype=data_type,\n",
    "                           task=task\n",
    "                          )    \n",
    "        # remove old fif file if it exists, and update bids_path\n",
    "        if str(out_path.fpath)[-len(epochs_suffix):] == epochs_suffix:\n",
    "            remove(out_path.fpath)\n",
    "            out_path = BIDSPath(root=derivatives_path, \n",
    "                       subject=subject[-3:], \n",
    "                       datatype=data_type,\n",
    "                       task=task\n",
    "                      )    \n",
    "        if op.exists(str(out_path.fpath)) == False:\n",
    "            Path(str(out_path.fpath)).mkdir(parents=True)\n",
    "\n",
    "        epochs_clean.save(str(out_path.fpath) + epochs_suffix, \n",
    "                        overwrite=True)\n",
    "\n",
    "        # add epochs to report\n",
    "        report.add_epochs(epochs_clean,  \n",
    "                        title='Epochs'\n",
    "                        )\n",
    "        ### Save plot of average across all trials\n",
    "        fig = epochs_clean.copy().average().plot(spatial_colors=True, \n",
    "                                                show=False);\n",
    "        report.add_figure(fig=fig, title='Grand average over all epochs')\n",
    "        plt.close(fig)\n",
    "        \n",
    "        # Add plots of average of each condition\n",
    "        for condition in event_id.values():\n",
    "            fig = epochs_clean[condition].copy().average().plot(spatial_colors=True, \n",
    "                                                                show=False);\n",
    "            report.add_figure(fig=fig, title=str(condition))\n",
    "            plt.close(fig)\n",
    "        \n",
    "        proc_time = time() - start_time\n",
    "        print('Total processing time: ', proc_time)\n",
    "\n",
    "        ### Report on how much was rejected\n",
    "        rm_epochs = epochs.selection.shape[0] - epochs_clean.selection.shape[0]\n",
    "        pct_epochs = rm_epochs / epochs.selection.shape[0] * 100   \n",
    "        rej_log_list.append(pd.DataFrame({'id':subject, \n",
    "                                        'cpu_time':proc_time,\n",
    "                                        'ntrials_rej':rm_epochs,\n",
    "                                        '%t_rej':round(pct_epochs, 2),\n",
    "                                        'ic_rm':len(ica.exclude),\n",
    "                                        'n_interp':str(ar.n_interpolate_['eeg'])\n",
    "                                        }, index=[0]\n",
    "                                        )\n",
    "                        )\n",
    "\n",
    "        # Save report to file\n",
    "        report_name = report_path + '/' + subject + '.html'\n",
    "        report.save(report_name, overwrite=True)\n",
    "    \n",
    "# Collate report logs\n",
    "rej_log = pd.concat(rej_log_list)\n",
    "rej_log.to_csv(report_path + '/rejection_log_all_Ss.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43464407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc55087",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "760055932674735d287fd612619c18ffc3840c7c49c197eeb438d57975a1e213"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
